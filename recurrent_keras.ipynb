{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as math\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "#import argparse\n",
    "#from RNN_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing arguments for Network definition\n",
    "\n",
    "#ap = argparse.ArgumentParser()\n",
    "#ap.add_argument('-data_dir', default='./data/test.txt')\n",
    "#ap.add_argument('-batch_size', type=int, default=50)\n",
    "#ap.add_argument('-layer_num', type=int, default=2)\n",
    "#ap.add_argument('-seq_length', type=int, default=50)\n",
    "#ap.add_argument('-hidden_dim', type=int, default=500)\n",
    "#ap.add_argument('-generate_length', type=int, default=500)\n",
    "#ap.add_argument('-nb_epoch', type=int, default=20)\n",
    "#ap.add_argument('-mode', default='train')\n",
    "#ap.add_argument('-weights', default='')\n",
    "#args = vars(ap.parse_args())\n",
    "\n",
    "DATA_DIR = \"resources/Delver_Magic_ALL.txt\"\n",
    "BATCH_SIZE = 50\n",
    "LAYER_NUM = 2\n",
    "SEQ_LENGTH = 50\n",
    "HIDDEN_DIM = 500\n",
    "GENERATE_LENGTH = 500\n",
    "NB_EPOCH = 20\n",
    "MODE = \"train\"\n",
    "WEIGHTS = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "# method for generating text\n",
    "def generate_text(model, length, vocab_size, ix_to_char):\n",
    "\t# starting with random character\n",
    "\tix = [np.random.randint(vocab_size)]\n",
    "\ty_char = [ix_to_char[ix[-1]]]\n",
    "\tX = np.zeros((1, length, vocab_size))\n",
    "\tfor i in range(length):\n",
    "\t\t# appending the last predicted character to sequence\n",
    "\t\tX[0, i, :][ix[-1]] = 1\n",
    "\t\tprint(ix_to_char[ix[-1]], end=\"\")\n",
    "\t\tix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "\t\ty_char.append(ix_to_char[ix[-1]])\n",
    "\treturn ('').join(y_char)\n",
    "\n",
    "# method for preparing the training data\n",
    "def load_data(data_dir, seq_length):\n",
    "\tdata = open(data_dir, 'r').read()\n",
    "\tchars = list(set(data))\n",
    "\tVOCAB_SIZE = len(chars)\n",
    "\n",
    "\tprint('Data length: {} characters'.format(len(data)))\n",
    "\tprint('Vocabulary size: {} characters'.format(VOCAB_SIZE))\n",
    "\n",
    "\tix_to_char = {ix:char for ix, char in enumerate(chars)}\n",
    "\tchar_to_ix = {char:ix for ix, char in enumerate(chars)}\n",
    "\n",
    "\tX = np.zeros((math.ceil(len(data)/seq_length), seq_length, VOCAB_SIZE))\n",
    "\ty = np.zeros((math.ceil(len(data)/seq_length), seq_length, VOCAB_SIZE))\n",
    "                   \n",
    "\tfor i in range(0, math.floor(len(data)/seq_length)):\n",
    "\t\tX_sequence = data[i*seq_length:(i+1)*seq_length]\n",
    "\t\tX_sequence_ix = [char_to_ix[value] for value in X_sequence]\n",
    "\t\tinput_sequence = np.zeros((seq_length, VOCAB_SIZE))\n",
    "\t\tfor j in range(seq_length):\n",
    "\t\t\tinput_sequence[j][X_sequence_ix[j]] = 1.\n",
    "\t\t\tX[i] = input_sequence\n",
    "\n",
    "\t\ty_sequence = data[i*seq_length+1:(i+1)*seq_length+1]\n",
    "\t\ty_sequence_ix = [char_to_ix[value] for value in y_sequence]\n",
    "\t\ttarget_sequence = np.zeros((seq_length, VOCAB_SIZE))\n",
    "\t\tfor j in range(seq_length):\n",
    "\t\t\ttarget_sequence[j][y_sequence_ix[j]] = 1.\n",
    "\t\t\ty[i] = target_sequence\n",
    "\treturn X, y, VOCAB_SIZE, ix_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a854b3091182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Creating training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix_to_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEQ_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_data' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "# Creating training data\n",
    "X, y, VOCAB_SIZE, ix_to_char = load_data(DATA_DIR, SEQ_LENGTH)\n",
    "\n",
    "\n",
    "print(X[0:50])\n",
    "print(y[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d2c5cba48ecd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "X=\"hello world\"\n",
    "y=\"mama is the best\"\n",
    "VOCAB_SIZE = 187678\n",
    "ix_to_char = [\"h\",\"1\"]\n",
    "\n",
    "print(y[0:50])\n",
    "print(X[0:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and compiling the Network\n",
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "  model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some sample before training to know how bad it is!\n",
    "generate_text(model, args['generate_length'], VOCAB_SIZE, ix_to_char)\n",
    "\n",
    "if not WEIGHTS == '':\n",
    "  model.load_weights(WEIGHTS)\n",
    "  nb_epoch = int(WEIGHTS[WEIGHTS.rfind('_') + 1:WEIGHTS.find('.')])\n",
    "else:\n",
    "  nb_epoch = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training if there is no trained weights specified\n",
    "if args['mode'] == 'train' or WEIGHTS == '':\n",
    "  while True:\n",
    "    print('\\n\\nEpoch: {}\\n'.format(nb_epoch))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, nb_epoch=1)\n",
    "    nb_epoch += 1\n",
    "    generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)\n",
    "    if nb_epoch % 10 == 0:\n",
    "      model.save_weights('checkpoint_layer_{}_hidden_{}_epoch_{}.hdf5'.format(LAYER_NUM, HIDDEN_DIM, nb_epoch))\n",
    "\n",
    "# Else, loading the trained weights and performing generation only\n",
    "elif WEIGHTS == '':\n",
    "  # Loading the trained weights\n",
    "  model.load_weights(WEIGHTS)\n",
    "  generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)\n",
    "  print('\\n\\n')\n",
    "else:\n",
    "  print('\\n\\nNothing to do!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
